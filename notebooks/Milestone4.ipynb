{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fc43ae",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea8629",
   "metadata": {},
   "source": [
    "***Milestone 4:*** In this milestone, you will deploy the machine learning model you trained in milestone 3.\n",
    "\n",
    "You might want to go over [this sample project](https://github.ubc.ca/mds-2021-22/DSCI_525_web-cloud-comp_students/blob/master/release/milestone4/sampleproject.ipynb) and get it done before starting this milestone.\n",
    "\n",
    "Milestone 4 checklist :\n",
    "\n",
    "- [x] Use an EC2 instance.\n",
    "- [x] Develop your API here in this notebook.\n",
    "- [x] Copy it to ```app.py``` file in EC2 instance.\n",
    "- [x] Run your API for other consumers and test among your colleagues.\n",
    "- [x] Summarize your journey.\n",
    "\n",
    "In this milestone, you will do certain things that you learned. For example...\n",
    "- Login to the instance\n",
    "- Work with Linux and use some basic commands\n",
    "- Configure security groups so that it accepts your webserver requests from your laptop\n",
    "- Configure AWS CLI\n",
    "\n",
    "In some places, I explicitly mentioned these to remind you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c281967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the packages that you need\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959bcc5",
   "metadata": {},
   "source": [
    "## 1. Develop your API\n",
    "\n",
    "rubric={mechanics:45}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608e655",
   "metadata": {},
   "source": [
    "You probably got how to set up primary URL endpoints from the [sampleproject.ipynb](https://github.ubc.ca/mds-2021-22/DSCI_525_web-cloud-comp_students/blob/master/release/milestone4/sampleproject.ipynb) and have them process and return some data. Here we are going to create a new endpoint that accepts a POST request of the features required to run the machine learning model that you trained and saved in last milestone (i.e., a user will post the predictions of the 25 climate model rainfall predictions, i.e., features,  needed to predict with your machine learning model). Your code should then process this data, use your model to make a prediction, and return that prediction to the user. To get you started with all this, I've given you a template that you should fill out to set up this functionality:\n",
    "\n",
    "***NOTE:*** You won't be able to test the flask module (or the API you make here) unless you go through steps in ```2. Deploy your API```. However, you can make sure that you develop all your functions and inputs properly here.\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "## Import any other packages that are needed\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 1. Load your model here\n",
    "model = joblib.load(\"model.joblib\")\n",
    "\n",
    "# 2. Define a prediction function\n",
    "def return_prediction(input_data):\n",
    "\n",
    "    # format input_data here so that you can pass it to model.predict()\n",
    "    data = np.array(input_data[\"data\"])\n",
    "    \n",
    "    # Add dummy dimension if 1D array\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "    \n",
    "    return model.predict(data).tolist()\n",
    "\n",
    "# 3. Set up home page using basic html\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    # feel free to customize this if you like\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to our rain prediction service</h1>\n",
    "    To use this service, make a JSON post request to the /predict url with 25 climate model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "# 4. define a new route which will accept POST requests and return model predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def rainfall_prediction():\n",
    "    content = request.json  # this extracts the JSON content we sent\n",
    "    prediction = return_prediction(content)\n",
    "    results = {\"Output\": prediction}  # return whatever data you wish, it can be just the prediction\n",
    "                     # or it can be the prediction plus the input data, it's up to you\n",
    "    return jsonify(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c18ff",
   "metadata": {},
   "source": [
    "## 2. Deploy your API\n",
    "\n",
    "rubric={mechanics:40}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d45379",
   "metadata": {},
   "source": [
    "Once your API (app.py) is working, we're ready to deploy it! For this, do the following:\n",
    "\n",
    "1. Setup an EC2 instance. Make sure you add a rule in security groups to accept `All TCP` connections from `Anywhere`. SSH into your EC2 instance from milestone2.\n",
    "2. Make a file `app.py` file in your instance and copy what you developed above in there. \n",
    "\n",
    "    2.1 You can use the Linux editor using ```vi```. More details on vi Editor [here](https://www.guru99.com/the-vi-editor.html). Use your previous learnings, notes, mini videos, etc. You can copy code from your jupyter and paste it into `app.py`.\n",
    "    \n",
    "    2.2 Or else you can make a file in your laptop called app.py and copy it over to your EC2 instance using ```scp```. Eg: ```scp -r -i \"ggeorgeAD.pem\" ~/Desktop/app.py  ubuntu@ec2-xxx.ca-central-1.compute.amazonaws.com:~/```\n",
    "\n",
    "3. Download your model from s3 to your EC2 instance. You want to configure your S3 for this. Use your previous learnings, notes, mini videos, etc.\n",
    "4. You should use one of those package managers to install the dependencies of your API, like `flask`, `joblib`, `sklearn`, etc...\n",
    "\n",
    "    4.1. (Additional help) you can install the required packages inside your terminal.\n",
    "        - Install conda:\n",
    "            wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "            bash Miniconda3-latest-Linux-x86_64.sh\n",
    "        - Install packages (there might be others): \n",
    "            conda install flask scikit-learn joblib\n",
    "\n",
    "5. Now you're ready to start your service, go ahead and run `flask run --host=0.0.0.0 --port=8080`. This will make your service available at your EC2 instance's `Public IPv4 address` on port 8080. Please ensure that you run this from where ```app.py``` and ```model.joblib``` reside.\n",
    "6. You can now access your service by typing your EC2 instances `public IPv4 address` append with `:8080` into a browser, so something like `http://Public IPv4 address:8080`. From step 4, you might notice that flask output saying \"Running on http://XXXX:8080/ (Press CTRL+C to quit)\", where XXXX is `Private IPv4 address`, and you want to replace it with the `Public IPv4 address`\n",
    "7. You should use `curl` to send a post request to your service to make sure it's working as expected.\n",
    ">EG: curl -X POST http://your_EC2_ip:8080/predict -d '{\"data\":[1,2,3,4,53,11,22,37,41,53,11,24,31,44,53,11,22,35,42,53,12,23,31,42,53]}' -H \"Content-Type: application/json\"\n",
    "\n",
    "8. Now, what happens if you exit your connection with the EC2 instance? Can you still reach your service?\n",
    "9. We could use several options to help us persist our server even after we exit our shell session. We'll be using `screen`. `screen` will allow us to create a separate session within which we can run `flask` and won't shut down when we exit the main shell session. Read [this](https://linuxize.com/post/how-to-use-linux-screen/) to learn more on ```screen```.\n",
    "10. Now, create a new `screen` session (think of this as a new, separate shell), using: `screen -S myapi`. If you want to list already created sessions do ```screen -list```. If you want to get into an existing ```screen -x myapi```.\n",
    "11. Within that session, start up your flask app. You can then exit the session by pressing `Ctrl + A then press D`. Here you are detaching the session, once you log back into EC2 instance you can attach it using ```screen -x myapi```.\n",
    "12. Feel free to exit your connection with the EC2 instance now and try reaccessing your service with `curl`. You should find that the service has now persisted!\n",
    "13. ***CONGRATULATIONS!!!*** You have successfully got to the end of our milestones. Move to Task 3 and submit it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736fbf7c",
   "metadata": {},
   "source": [
    "## 3. Summarize your journey from Milestone 1 to Milestone 4\n",
    "rubric={mechanics:10}\n",
    ">There is no format or structure on how you write this. (also, no minimum number of words).  It's your choice on how well you describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a88bd-51ea-4d13-b637-daf771b2cbef",
   "metadata": {},
   "source": [
    "The journey from Milestone 1 to Milestone 4 has been exhilarating! Our team comprised of members who do not have any background in cloud computing. Thus, our learning from this course, particularly from the targets of the four milestones, has been very significant. We all now have a decent understanding of how to use AWS. Furthermore, our team benefited from working together in a cloud-computing set-up, which is the reality of workplaces in a post-pandemic world, particularly in the technology sector. More specifically, the ability to share computational resources alongside understanding how the security protocols work will be extremely useful as we transition from academia to work in a couple of months.\n",
    "\n",
    "**Milestone 1:** The main objective of Milestone 1 was to download a very large dataset (magnitude of ~6 GB) using an API, combine these files (which were in different formats), and generally ease working with big data for analytical purposes in the absence of cloud computing. We compared different approaches to accomplish the goal (Parquet, Feather, Arrow Exchange), alongside comparing the time it took to undertake each task on our individual machines. Interestingly, half our team had PCs and the other half had Mac’s, facilitating a reasonable comparison of loading times. Our immediate observation was how these new formats are better optimized for loading and saving data compared to the usual data file formats we have been exposed to in MDS until Block 5. Finally, we used Apache Arrow to load data into R and undertake exploratory data analysis in our notebook using the combined data.  For all this to run smoothly, we dropped some columns, changed data type (from float64 to float32), and performed other wrangling tasks which was able to reduce our memory usage significantly to around ~1.5 GB. We were already quite impressed with the computational speed gains with these different file formats but settled for Arrow Exchange since we had already trimmed our dataset to relevant columns, found installation easy, didn’t require reloading of data. Furthermore, Arrow Exchange worked with dplyr functions for doing a portion of the milestone deliverable in R, and our team overall found it more time efficient than the other options. However, at this stage, our curiosity for how much more time efficiency gains can be made using cloud computing was fully piqued.\n",
    "\n",
    "**Milestone 2:** The goal of the milestone 2 was to prepare and wrangle our rainfall data for the machine learning task to be performed in milestone 3. For this milestone, our team migrated to the AWS cloud where we setup EC2 instance, JupyterHub, and S3 bucket in a collaborative environment, alongside adding access for each team member using their public keys. Then, we copied API data script from Milestone 1 in JupyterHub to save combined rainfall data in a shared data folder which was accessible to all team members. Next, we moved the combined rainfall data from the shared folder to output folder in S3 bucket, which enabled all members to have access to it through their AWS CLI in their individual notebook workspace in JupyterHub. Post loading the combined data in Jupyter notebook, we performed data wrangling such as pivoting and grouping to get rainfall data for Sydney as specified in the requirement for Milestone 2. Finally, we saved the data back in the output folder of the S3 bucket for future use.\n",
    "\n",
    "**Milestone 3:** In milestone 3, we setup an EMR cluster to train our model using data from the S3 bucket that was created in Milestone 2. This permitted us to use Spark as a kernel on JupyterHub alongside performing hyperparameter tuning. To enable this work, we had to set up a proxy connection through FoxyProxy which allowed us to use JupyterHub on the EMR cluster using our browser. Even though we only used 1 master node in the EMR cluster, it gave us the foundation to use machine learning on data that would otherwise be too large to fit within the memory. We set up a RandomForestRegressor scikit-learn model, and to obtain the best hyperparameter in Spark, we used a Spark Machine Learning library called MLlib. We also learnt how to store or serialize our trained model using joblib, which will be used for deployment in milestone 4. Interestingly, syntax of PySpark was different to what we have been learning in Python, so we certainly picked up some new skills.\n",
    "\n",
    "**Milestone 4:** In Milestone 4, we deployed the machine learning model from Milestone 3 as a REST API. This allows other users to utilize the services we enabled as URL endpoints. Specifically, we created an endpoint that accepts POST requests with features and return the corresponding predictions computed by the machine learning model from Milestone 3. To accomplish this task, we developed a simple web service with Flask where we defined a predict function that given a set of features returns the predictions computed by our machine learning model from Milestone 3. Then, we added an endpoint for accepting POST requests from users to pass input features to the predict function and return its results. To make this service accessible to users we deployed the API on an EC2 instance by downloading our machine learning model, installing dependencies, creating the entry point (app.py) and starting the service. Finally, we made the service persistent.\n",
    "\n",
    "Overall, our team has greatly benefited from this course. Both the lectures and the milestones have provided us a lot of useful information to get comfortable in using AWS cloud solutions. Along with all that we have learned thus far in MDS, we feel more confident in our abilities to act as an independent individual in a corporate environment where cloud computing is the backbone of all the analytical work undertaken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fe94a",
   "metadata": {},
   "source": [
    "## 4. Submission instructions\n",
    "rubric={mechanics:5}\n",
    "\n",
    "In the textbox provided on Canvas please put a link where TAs can find the following-\n",
    "- [x] This notebook with solution to ```1 & 3```\n",
    "- [x] Screenshot from \n",
    "    - [x] Output after trying curl. Here is a [sample](https://github.ubc.ca/mds-2021-22/DSCI_525_web-cloud-comp_students/blob/master/release/milestone4/images/curl_deploy_sample.png). This is just an example; your input/output doesn't have to look like this, you can design the way you like. But at a minimum, it should show your prediction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d02d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
